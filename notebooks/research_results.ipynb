{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JudgeAgent Scoring Analysis — Research Results\n",
    "\n",
    "This notebook presents the findings from our sensitivity analysis of the JudgeAgent's\n",
    "weighted-sum scoring algorithm. We analyze 221 real judge decisions from a Pantheon-to-Vatican\n",
    "walking tour to understand how parameter changes affect content selection.\n",
    "\n",
    "**Research questions:**\n",
    "1. How does the scoring algorithm distribute scores across content types?\n",
    "2. How sensitive is content selection to type-preference weights?\n",
    "3. How sensitive is content selection to the relevance multiplier?\n",
    "4. What are the implications for algorithm tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "RESULTS = Path('..') / 'results'\n",
    "METRICS = RESULTS / 'metrics'\n",
    "FIGURES = RESULTS / 'figures'\n",
    "\n",
    "# Load metrics\n",
    "with open(METRICS / 'baseline_metrics.json') as f:\n",
    "    baseline = json.load(f)\n",
    "with open(METRICS / 'type_preference_sensitivity.json') as f:\n",
    "    type_sens = json.load(f)\n",
    "with open(METRICS / 'relevance_sensitivity.json') as f:\n",
    "    rel_sens = json.load(f)\n",
    "\n",
    "print('Metrics loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Score Distribution\n",
    "\n",
    "The JudgeAgent uses a weighted-sum model with 6 criteria:\n",
    "- **Content quality** (30 pts): Penalizes short/empty titles and descriptions\n",
    "- **Title relevance** (20 pts): Keyword overlap between title and location name\n",
    "- **Description relevance** (15 pts): Keyword overlap in description\n",
    "- **Keyword overlap** (20 pts max): `min(overlap_count * 5, 20)`\n",
    "- **Type preference** (10/5/5): Text=10, Video=5, Music=5\n",
    "- **URL quality** (5 pts): Valid URL bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total judge decisions analyzed: {baseline['total_judgments']}\")\n",
    "print(f\"\\nAverage scores by content type:\")\n",
    "for ct, score in baseline['type_avg_scores'].items():\n",
    "    print(f\"  {ct:8s}: {score:.1f}/100\")\n",
    "\n",
    "print(f\"\\nWin rates (% of times selected as best):\")\n",
    "for ct, rate in baseline['type_win_rates'].items():\n",
    "    print(f\"  {ct:8s}: {rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nAverage score gap (winner - runner-up): {baseline['avg_score_gap']:.1f} pts\")\n",
    "\n",
    "print(f\"\\nDetailed statistics:\")\n",
    "for ct, details in baseline['type_score_details'].items():\n",
    "    print(f\"  {ct:8s}: n={details['count']:3d}, min={details['min']:.0f}, max={details['max']:.0f}, avg={details['avg']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(FIGURES / 'score_distribution.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(FIGURES / 'win_rate_pie.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations — Baseline\n",
    "\n",
    "- **Text dominates** with 86.4% win rate and highest average score (87.0)\n",
    "- **Video is competitive** with 81.9 avg score but only 10.0% win rate\n",
    "- **Music lags significantly** at 61.1 avg score and 3.6% win rate\n",
    "- **Score gap of 21.0 pts** indicates confident decisions (not close calls)\n",
    "- Text scores range 80-100 while music is stuck at 60-65 — a narrow, low band\n",
    "\n",
    "The text-agent advantage comes from Wikipedia's rich, keyword-dense content that scores\n",
    "highly on both content quality and relevance criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: Type Preference Sensitivity\n",
    "\n",
    "We sweep type-preference weights across 6 configurations to measure how much\n",
    "the 10/5/5 default (favoring text) affects outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Config':<30s}  {'Text%':>6s}  {'Video%':>6s}  {'Music%':>6s}\")\n",
    "print('-' * 56)\n",
    "for r in type_sens:\n",
    "    print(f\"{r['config']:<30s}  {r['text_pct']:6.1f}  {r['video_pct']:6.1f}  {r['music_pct']:6.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(FIGURES / 'type_preference_sensitivity.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations — Type Preference Sensitivity\n",
    "\n",
    "| Finding | Detail |\n",
    "|---------|--------|\n",
    "| Text remains dominant in all configs | Even with no preference (0/0/0), text wins 80.9% |\n",
    "| Removing text bonus costs ~9 ppts | Default 89.5% → Equal 80.9% |\n",
    "| Music needs +15 to appear at all | Music-boosted (5/5/15) gives music only 6.2% |\n",
    "| Video can reach 23% with +15 boost | But still far below text |\n",
    "\n",
    "**Conclusion:** Type preference is a secondary factor. The 5-10 point preference bonus\n",
    "is overshadowed by the ~26 point content quality gap between text and music.\n",
    "To meaningfully diversify content selection, type-preference alone is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 2: Relevance Multiplier Sensitivity\n",
    "\n",
    "The relevance component uses `min(keyword_overlap * multiplier, 20)` where default\n",
    "multiplier = 5. We sweep from 0 (disable relevance) to 10 (double weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Multiplier':>10s}  {'Text%':>6s}  {'Video%':>6s}  {'Music%':>6s}  {'Avg Gap':>8s}\")\n",
    "print('-' * 44)\n",
    "for r in rel_sens:\n",
    "    print(f\"{r['multiplier']:10d}  {r['text_pct']:6.1f}  {r['video_pct']:6.1f}  {r['music_pct']:6.1f}  {r['avg_gap']:8.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(FIGURES / 'relevance_sensitivity.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations — Relevance Sensitivity\n",
    "\n",
    "| Finding | Detail |\n",
    "|---------|--------|\n",
    "| **Most sensitive parameter** | Text win rate swings from 74.6% (mult=0) to 89.5% (mult=1-8) |\n",
    "| Disabling relevance (mult=0) | Music appears at 6.2%, video rises to 19.1% |\n",
    "| High relevance (mult=10) | Inverts — text drops to 80.9%, video rises to 19.1% |\n",
    "| Judge confidence increases linearly | Score gap: 9.7 (mult=0) → 24.5 (mult=8) |\n",
    "| Sweet spot at mult=1-8 | Stable text dominance with clear confidence |\n",
    "\n",
    "**Conclusion:** The relevance multiplier is the most influential tuning knob.\n",
    "At mult=0, the algorithm loses its ability to reward location-specific content,\n",
    "and selection becomes more random. At mult=10, over-weighting relevance slightly\n",
    "hurts text (which already has high relevance) due to the cap at 20 points.\n",
    "The default mult=5 sits in the stable plateau — a robust choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Recommendations\n",
    "\n",
    "### Research Findings\n",
    "\n",
    "1. **Text dominance is structural**, not an artifact of tuning. Wikipedia's rich content\n",
    "   naturally scores high on content quality (30 pts) and description relevance (15 pts).\n",
    "\n",
    "2. **Type preference weights have limited impact** (~9 ppts swing). They act as a\n",
    "   tiebreaker, not a selection driver.\n",
    "\n",
    "3. **Relevance multiplier is the most sensitive parameter** (15 ppt swing). It controls\n",
    "   how strongly the algorithm favors location-specific content.\n",
    "\n",
    "4. **Default parameters are well-chosen.** They sit in the stable region of the parameter\n",
    "   space with robust text selection and clear confidence margins.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "| Recommendation | Rationale |\n",
    "|---------------|----------|\n",
    "| Keep defaults (mult=5, text pref=10) | Stable, well-tested operating point |\n",
    "| Add diversity bonus for multi-modal tours | Current algorithm naturally clusters on text |\n",
    "| Consider MMR-style re-ranking | Maximal Marginal Relevance could balance relevance + diversity |\n",
    "| Improve Spotify metadata | Music's narrow 60-65 score band suggests poor search-to-location matching |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}